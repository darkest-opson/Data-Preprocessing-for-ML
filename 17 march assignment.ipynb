{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b55751-af7e-4466-b1d1-e6990c4dfd16",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a872b-f0ed-449d-b181-e75653c24415",
   "metadata": {},
   "source": [
    "Missing data could result from a human factor (for example, a person deliberately failing to respond to a survey question), a problem in electrical sensors, or other factors.Mostly it is written as NaN in Dataframes.\n",
    "\n",
    "Many machine learning algorithms fail if the dataset contains missing values. However, algorithms like K-nearest and Naive Bayes support data with missing values. You may end up building a biased machine learning model, leading to incorrect results if the missing values are not handled properly.\n",
    "\n",
    "There are several algorithms that are not affected by missing values:\n",
    "\n",
    "Decision Trees: \n",
    "\n",
    "Decision trees can handle missing values in a dataset by using surrogate splits. When a variable is missing, the decision tree algorithm will use another variable that is highly correlated with the missing variable to create a split.\n",
    "\n",
    "Random Forest: \n",
    "\n",
    "Random Forest algorithm is an ensemble algorithm that can handle missing values in the dataset. It uses multiple decision trees to make a prediction and can handle missing values using surrogate splits.\n",
    "\n",
    "K-Nearest Neighbors (KNN):\n",
    "\n",
    "KNN algorithm can handle missing values in the dataset by using the mean or median values of the nearest neighbors.\n",
    "\n",
    "Naive Bayes:\n",
    "\n",
    "Naive Bayes algorithm can handle missing values in the dataset by ignoring the missing values while calculating the probabilities.\n",
    "\n",
    "Support Vector Machines (SVM):\n",
    "\n",
    "SVM algorithm can handle missing values in the dataset by finding a hyperplane that separates the data points with the highest margin, and ignoring the missing values.\n",
    "\n",
    "Principal Component Analysis (PCA): \n",
    "\n",
    "PCA is a dimensionality reduction technique that can handle missing values in the dataset by using the mean or median values of the available data to impute the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071fc92-a948-4253-a08f-b4ce84c5e98f",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa7b3-ef8e-4291-b8be-8a7ebf567c53",
   "metadata": {},
   "source": [
    "Techniques :\n",
    "\n",
    "1: mean imputation\n",
    "\n",
    "2: median imputation\n",
    "\n",
    "3: mode imputation\n",
    "\n",
    "4: delete the whole row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bbc2b3-4e64-40eb-8323-070f224a94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61453c60-0b41-4312-b74b-c970d28c7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ebf57d-7fdb-4af5-b2de-3beb311d2a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becf5617-674f-4a53-8df8-e69139e5d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = df[['age']].mean()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2acd988f-a842-4d0f-b74d-0a23295d81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean value imputation\n",
    "df['age'] = df['age'].fillna(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6332e9eb-91f4-47e4-916e-d3b09133b78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41649b00-9e7a-4dcc-9b10-1c2882c61b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65c87a21-60bc-47c4-8c1f-b9860f766f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34e4b953-9d55-4200-8fc8-5b2c2b010fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median imputaion\n",
    "\n",
    "df['age'] = df['age'].fillna(df['age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d25ac639-f7d2-4967-96d9-31fdf662dc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d1c5965-8345-4c79-ad79-bcd03060cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "764eb116-1d40-424e-a714-3358fbc19279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bec6bd8a-7217-4826-883d-d4f715d293f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode value imputation\n",
    "df['age'] = df['age'].fillna(df['age'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44d34a69-ff67-4f28-be92-b8776c5b895c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70708e94-eeb4-4802-8800-c80ed8d07f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the row having missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a464f00-e7b9-42de-a0b0-2bdae154ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3380a05-0bfb-4c0f-892c-901f9a396a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c0c28e0-e636-4bf5-be7d-c2e05191c2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived       0\n",
       "pclass         0\n",
       "sex            0\n",
       "age            0\n",
       "sibsp          0\n",
       "parch          0\n",
       "fare           0\n",
       "embarked       0\n",
       "class          0\n",
       "who            0\n",
       "adult_male     0\n",
       "deck           0\n",
       "embark_town    0\n",
       "alive          0\n",
       "alone          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a1758-4a45-4a97-8682-0ebbaf04d0bd",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a0d01-fc22-491c-8229-e9d431257750",
   "metadata": {},
   "source": [
    "Imbalanced data is a term used to describe a situation in which the distribution of the target variable in a dataset is heavily skewed towards one class or a few classes, while the other classes have very few instances. For example, a dataset of customer churn prediction may have only a few instances of customers who churned, while the majority of customers remain loyal.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several problems during the training of machine learning models, such as:\n",
    "\n",
    "1: Biased model performance: Due to the imbalance in the dataset, the model can become biased towards the majority class and may perform poorly on the minority class.\n",
    "\n",
    "2: Poor generalization: Since the minority class has very few instances, the model may not learn enough about it to generalize well to new, unseen data.\n",
    "\n",
    "3: Misleading accuracy: Accuracy can be a misleading metric to evaluate model performance on imbalanced data since the model can achieve high accuracy by simply predicting the majority class.\n",
    "\n",
    "To address imbalanced data, several techniques can be used, such as:\n",
    "\n",
    "1: Resampling: This involves either oversampling the minority class or undersampling the majority class to balance the dataset.\n",
    "\n",
    "2: Cost-sensitive learning: This involves assigning different misclassification costs to different classes during the training of the model.\n",
    "\n",
    "3: Ensemble methods: This involves combining several models trained on balanced subsets of the data to improve the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc113537-6321-4484-8d90-c821ac8abac9",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "Up-sampling and down-sampling are two common techniques used to balance imbalanced datasets by either increasing or decreasing the number of instances in a particular class.\n",
    "\n",
    "Up-sampling is a technique of increasing the number of instances of the minority class to balance the dataset with the majority class. This can be done by duplicating instances from the minority class or by generating new instances using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "For example, let's say we have a dataset of customer churn prediction, and the churned customers are a minority class. In this case, we can up-sample the churned customers to create more instances and balance the dataset with the non-churned customers.\n",
    "\n",
    "Down-sampling is a technique of reducing the number of instances in the majority class to balance the dataset with the minority class. This can be done by randomly selecting a subset of instances from the majority class.\n",
    "\n",
    "For example, suppose we have a dataset of credit card fraud detection, and the fraudulent transactions are a minority class. In this case, we can down-sample the non-fraudulent transactions to balance the dataset with the fraudulent transactions.\n",
    "\n",
    "When to use up-sampling and down-sampling?\n",
    "\n",
    "Up-sampling is useful when the dataset has a small number of instances in the minority class and sufficient data are available to up-sample the minority class without overfitting.\n",
    "Down-sampling is useful when the dataset is too large, and the computational cost of training the model is high or when the majority class has too many instances that dominate the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bfaec-aac7-4b57-baa2-9741bc50bf5a",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new data points based on existing ones. The goal of data augmentation is to increase the diversity of the dataset to improve the model's performance.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to address imbalanced datasets. It is a type of oversampling technique that creates synthetic instances of the minority class by interpolating between instances in the minority class.\n",
    "\n",
    "Here is how SMOTE works:\n",
    "\n",
    "SMOTE selects a random instance from the minority class.\n",
    "\n",
    "SMOTE selects k nearest neighbors of the selected instance from the minority class.\n",
    "\n",
    "SMOTE creates new instances by interpolating between the selected instance and its k nearest neighbors.\n",
    "\n",
    "The new instances are added to the dataset as synthetic instances of the minority class.\n",
    "\n",
    "For example, suppose we have a dataset of customer churn prediction, and the churned customers are a minority class. In this case, we can use SMOTE to create new synthetic instances of churned customers by interpolating between the existing churned customers in the dataset.\n",
    "\n",
    "SMOTE has several advantages over other data augmentation techniques, such as:\n",
    "\n",
    "It does not create exact copies of existing instances, which reduces the risk of overfitting.\n",
    "\n",
    "It can generate new instances that are different from the existing ones, which increases the diversity of the dataset.\n",
    "\n",
    "It can be combined with other data augmentation techniques like random rotations, flips, or zooms, to further increase the diversity of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2eeec-3b5a-40dd-9d0a-72c3e57dc285",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers are observations in a dataset that are significantly different from other observations in the same dataset. Outliers can be caused by measurement or data collection errors, or they can be valid observations that are simply extreme values.\n",
    "\n",
    "It is essential to handle outliers for several reasons:\n",
    "\n",
    "They can significantly affect the statistical measures used to describe the data, such as the mean and standard deviation, leading to biased estimates.\n",
    "\n",
    "They can affect the distribution of the data, leading to incorrect assumptions about the underlying distribution, such as normality.\n",
    "\n",
    "They can have a disproportionate impact on the model's performance, especially in regression models where they can cause the model to fit the outliers instead of the underlying pattern in the data.\n",
    "\n",
    "They can reduce the model's interpretability, as outliers may be seen as anomalies that do not fit the underlying pattern.\n",
    "\n",
    "To handle outliers, several techniques can be used, such as:\n",
    "\n",
    "Removing outliers: This involves removing the observations that are identified as outliers from the dataset. This can be done using statistical methods or domain knowledge.\n",
    "\n",
    "Winsorization: This involves replacing the extreme values with the values of the k-th largest or smallest observation in the dataset.\n",
    "\n",
    "Transformations: This involves applying mathematical transformations to the data, such as logarithmic or exponential transformations, to reduce the effect of extreme values.\n",
    "\n",
    "Robust models: This involves using robust statistical models that are less sensitive to outliers, such as the median instead of the mean or using non-parametric methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b579e3-db34-40d1-b484-40a08240b67d",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Handling missing data is an important step in data analysis, as missing data can lead to biased results, reduce the statistical power of the analysis, and even lead to incorrect conclusions. Here are some techniques that can be used to handle missing data:\n",
    "\n",
    "Deletion: This involves deleting the observations or variables that have missing data. This technique can be used when the amount of missing data is small, and the data is missing at random (MAR). There are three types of deletion techniques: listwise deletion (deleting all observations with missing data), pairwise deletion (deleting only the observations with missing data for the analysis), and casewise deletion (deleting only the variables with missing data for the analysis).\n",
    "\n",
    "Imputation: This involves filling in the missing values with estimated values. This technique can be used when the amount of missing data is large, or the data is not missing at random (MNAR). There are several imputation techniques, such as mean imputation (replacing the missing values with the mean of the available data), regression imputation (using a regression model to predict the missing values), and multiple imputation (generating multiple imputed datasets based on statistical models).\n",
    "\n",
    "Prediction models: This involves using a prediction model to estimate the missing data. This technique can be used when there is a strong relationship between the missing data and other variables in the dataset.\n",
    "\n",
    "Domain knowledge: This involves using domain knowledge to estimate the missing data. This technique can be used when the missing data can be reasonably estimated based on the knowledge of the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463c8da-d699-4750-8806-3b506b8859b6",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657caca-4d11-41f3-b9ce-3295e7d72103",
   "metadata": {},
   "source": [
    "Determining if the missing data is missing at random (MAR) or not missing at random (NMAR) is important because it can affect the choice of missing data handling techniques. Here are some strategies that can be used to determine if the missing data is MAR or NMAR:\n",
    "\n",
    "Visual inspection: This involves creating visualizations, such as histograms or scatter plots, to determine if there is a pattern to the missing data. If the missing data appears to be randomly distributed across the data, it is likely MAR. If there is a pattern to the missing data, such as missing data for a particular demographic or time period, it may be NMAR.\n",
    "\n",
    "Statistical tests: This involves conducting statistical tests, such as the Little's MCAR test, to determine if the missing data is MAR. This test compares the missing data pattern to a completely random missing data pattern. If the test results indicate that the missing data pattern is not significantly different from the completely random missing data pattern, it is likely MAR.\n",
    "\n",
    "Imputation methods: This involves using different imputation methods, such as mean imputation or multiple imputation, to fill in the missing data and comparing the results. If the results of different imputation methods are similar, it is likely MAR. If the results are significantly different, it may be NMAR.\n",
    "\n",
    "Domain knowledge: This involves using domain knowledge to determine if the missing data is likely MAR or NMAR. If there is a known reason for the missing data, such as a technical error or survey non-response, it is likely MAR. If there is no known reason for the missing data, it may be NMAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee88fa-3342-4fc3-9967-8343243eb94b",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dadb39-675c-4e6e-82e4-98bf2f6a41ba",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets, it can be challenging to evaluate the performance of machine learning models accurately. Here are some strategies that can be used to evaluate the performance of machine learning models on imbalanced datasets:\n",
    "\n",
    "Confusion matrix: The confusion matrix is a table that summarizes the performance of a machine learning model. It shows the number of true positives, true negatives, false positives, and false negatives. This can be used to calculate metrics such as precision, recall, F1 score, and accuracy.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve: The ROC curve is a graphical representation of the performance of a binary classifier at different thresholds. It shows the true positive rate (TPR) against the false positive rate (FPR) at different threshold values. The area under the ROC curve (AUC) can be used as a metric to evaluate the performance of the classifier.\n",
    "\n",
    "Precision-Recall (PR) Curve: The PR curve is a graphical representation of the trade-off between precision and recall at different threshold values. It shows the precision against the recall at different threshold values. The area under the PR curve (AUPRC) can be used as a metric to evaluate the performance of the classifier.\n",
    "\n",
    "Resampling techniques: Resampling techniques such as oversampling or undersampling can be used to balance the dataset. This can help to improve the performance of the classifier.\n",
    "\n",
    "Cost-sensitive learning: Cost-sensitive learning is a technique that assigns different costs to misclassification errors. This can help to improve the performance of the classifier by minimizing the cost of misclassification.\n",
    "\n",
    "Ensemble methods: Ensemble methods such as bagging or boosting can be used to improve the performance of the classifier on imbalanced datasets by combining multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bc5c4-f5cc-4e7f-80ad-fa278ec97f7b",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4b121-a800-4223-8937-9db2729d6850",
   "metadata": {},
   "source": [
    "When working with unbalanced datasets, there are different methods that can be employed to balance the dataset and down-sample the majority class. Here are some methods:\n",
    "\n",
    "Under-sampling: Under-sampling involves reducing the number of samples in the majority class to balance the dataset. This can be done randomly or using techniques such as Tomek links or Cluster Centroids.\n",
    "\n",
    "Over-sampling: Over-sampling involves increasing the number of samples in the minority class to balance the dataset. This can be done by replicating samples or using techniques such as Synthetic Minority Over-sampling Technique (SMOTE) or Adaptive Synthetic Sampling (ADASYN).\n",
    "\n",
    "Hybrid methods: Hybrid methods involve combining under-sampling and over-sampling techniques to balance the dataset. This can be done using techniques such as Synthetic Minority Over-sampling Technique followed by Tomek Links (SMOTETomek) or Synthetic Minority Over-sampling Technique followed by Neighborhood Cleaning Rule (SMOTENC).\n",
    "\n",
    "Weighting: Weighting involves assigning different weights to the samples in the dataset to balance the dataset. This can be done using techniques such as cost-sensitive learning or sample weighting.\n",
    "\n",
    "To down-sample the majority class, under-sampling or hybrid methods can be used. Here are some steps to perform under-sampling:\n",
    "\n",
    "Randomly select a subset of samples from the majority class to match the number of samples in the minority class.\n",
    "\n",
    "1: Train the machine learning model on the balanced dataset.\n",
    "\n",
    "2 :Evaluate the performance of the model on a separate test set.\n",
    "\n",
    "3: Repeat steps 1-3 multiple times to obtain an average estimate of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368b80c-299c-431f-82c0-b5e87e9f08f4",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07666bce-a7da-4a73-841d-952855c23aa9",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets with a low percentage of occurrences, there are different methods that can be employed to balance the dataset and up-sample the minority class. Here are some methods:\n",
    "\n",
    "Over-sampling: Over-sampling involves increasing the number of samples in the minority class to balance the dataset. This can be done by replicating samples or using techniques such as Synthetic Minority Over-sampling Technique (SMOTE) or Adaptive Synthetic Sampling (ADASYN).\n",
    "\n",
    "Hybrid methods: Hybrid methods involve combining under-sampling and over-sampling techniques to balance the dataset. This can be done using techniques such as Synthetic Minority Over-sampling Technique followed by Tomek Links (SMOTETomek) or Synthetic Minority Over-sampling Technique followed by Neighborhood Cleaning Rule (SMOTENC).\n",
    "\n",
    "Weighting: Weighting involves assigning different weights to the samples in the dataset to balance the dataset. This can be done using techniques such as cost-sensitive learning or sample weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b5313-b626-441a-a1a3-b938218c7593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
